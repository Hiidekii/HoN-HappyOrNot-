{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db5d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import face_recognition as fr\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from os import remove\n",
    "import collections\n",
    "import operations\n",
    "import operations_cutting\n",
    "face_cascade = cv2.CascadeClassifier('gui_2/assets/haarcascade_frontalface_default.xml')\n",
    "HEIGHT, WIDTH =48,48\n",
    "other_emotions =  []\n",
    "initial_emotions = ['sadness','happiness', 'surprise', 'neutral','fear', 'disgust','anger']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9ac7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"../data/all/train/\",\"../data/all/test/\"]#[\"data/CK_cut/\"]#[\"data/all/train/\",\"data/all/test/\"]# #[\"data/CK+/\",\"data/fer/train/\"]\n",
    "# data  = listdir(path)\n",
    "ignore = [\"morralla\",\".DS_Store\",\"contempt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fb9095",
   "metadata": {},
   "source": [
    "### With operations module we can load all images in the paths provided. We can load in a single objet data for train only 1 model or in 2 combined models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07f17214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Data object with all images in paths.\n",
    "dataset= operations.Datos(paths,initial_emotions,48,48)\n",
    "#Load image's directions\n",
    "dataset.imgs_load_initial()\n",
    "dataset.imgs_load_other()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2678ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Cantidad de datos por emocion\n",
    "data = collections.Counter(dataset.state)\n",
    "graph = plt.bar(data.keys(), data.values())   \n",
    "plt.title('imagenes en dataset')\n",
    "plt.xlabel('emotion')\n",
    "plt.ylabel('Qty')\n",
    "plt.show() #dibujamos el histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2120f443",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load images in a array\n",
    "dataset.image_to_data()\n",
    "dataset.image_to_data('other')\n",
    "#Convert the images in gray and dimensions provided\n",
    "dataset.data_conversion()\n",
    "dataset.data_conversion('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed51cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.other_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b8bfe",
   "metadata": {},
   "source": [
    "## With operations_cutting we can load images cutted with cv2.cascadeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9baad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cut = operations_cutting.Data_cut(paths,initial_emotions,48,48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ab43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cut.load_imgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cut.recortar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cut.imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879e1cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_cut.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b221d9",
   "metadata": {},
   "source": [
    "# Preparamos entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(dataset.state)\n",
    "y_dummies = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5764951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335e6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.imgs,y_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten,BatchNormalization,Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.convolutional import MaxPooling1D,MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from livelossplot.tf_keras import PlotLossesCallback\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b19d26d",
   "metadata": {},
   "source": [
    "# MODELO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751fe068",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# 1-conv\n",
    "model.add(Conv2D(64,(3,3),padding='same',input_shape = (48,48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 2-conv\n",
    "model.add(Conv2D(128,(5,5),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 3-conv\n",
    "model.add(Conv2D(512,(3,3),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 4-conv\n",
    "model.add(Conv2D(512,(3,3),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(len(set(dataset.state)),activation='softmax'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be8cea2",
   "metadata": {},
   "source": [
    "# MODELO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae2e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "#model.add(Flatten())\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(imgs_array.shape[1:])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(filters=512, kernel_size=3, activation='relu', input_shape=(imgs_array.shape[1:])))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation = 'relu'))\n",
    "model.add(Dense(128,activation = 'relu'))\n",
    "model.add(Dense(256,activation = 'relu'))\n",
    "model.add(Dense(512,activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(len(set(state), activation='softmax'))\n",
    "opt = Adam(learning_rate=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e870c9d",
   "metadata": {},
   "source": [
    "### Este codigo nos permite ver el progreso en tiempo real de los entrenamientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86542935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 200\n",
    "# # steps_per_epoch = train_generator.n//train_generator.batch_size\n",
    "# # validation_steps = validation_generator.n//validation_generator.batch_size\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\"modelo_ck_fc_tfeid_p1.h5\",monitor='val_accuracy',\n",
    "#                             save_weights_only=False,save_best_only=True, model='max',verbose=1)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=3,min_lr=0.000001,model='auto')\n",
    "\n",
    "# callbacks = [PlotLossesCallback(),checkpoint,reduce_lr]\n",
    "# history = model.fit(X_train,y_train,\n",
    "#     epochs=epochs,\n",
    "#     validation_data=(X_test,y_test),\n",
    "#     callbacks=callbacks\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed763968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model, model_from_json\n",
    "from keras import callbacks, optimizers\n",
    "import tensorflow as tf\n",
    "from datetime import date\n",
    "\n",
    "fecha=str(date.today().year)+str(date.today().month)+str(date.today().day)    \n",
    "symbol = 'no_dig_no_fear'\n",
    "h5 = symbol + fecha + '_v1.h5'\n",
    "checkpoint = callbacks.ModelCheckpoint(h5,\n",
    "                                       monitor='loss',\n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True,\n",
    "                                       #save_weights_only=True,\n",
    "                                       mode='auto',\n",
    "                                       save_freq=1)\n",
    "callback = [checkpoint]\n",
    "\n",
    "modelo = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs = 5000,callbacks = callback,validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0b3eda",
   "metadata": {},
   "source": [
    "## Operaciones para comprobar predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbceb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"data/predict/\"]\n",
    "# data_pred  = listdir(path)\n",
    "imgs_pred = []\n",
    "state_pred = []\n",
    "im_pred = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    for item in listdir(path):\n",
    "        if item not in ignore:\n",
    "            if item in initial_emotions:\n",
    "                imgs_pred.extend([{\"path\": f\"{path}{item}/{p}\", \"emotion\": get_emotion(item,True)}for p in listdir(f\"{path}{item}\")])\n",
    "                #state_pred.extend([item for p in listdir(f\"{path}{item}\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d91a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f61c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777cc70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('intial_emotions_2_20211116_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22327b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_pred_ = []\n",
    "state_pred = []\n",
    "for p in imgs_pred:\n",
    "    temp = cv2.imread(p[\"path\"],0)\n",
    "    faces = face_cascade.detectMultiScale(temp, 1.05, 5)\n",
    "    if len(faces)>0:\n",
    "        for (x,y,w,h) in faces:\n",
    "            recortada = temp[y:y+h, x:x+w]\n",
    "            recortada = cv2.resize(recortada,(HEIGHT, WIDTH))\n",
    "            imgs_pred_.append(recortada)\n",
    "            state_pred.append(p[\"emotion\"])\n",
    "    \n",
    "        \n",
    "imgs_pred_ = [el/255 for el in imgs_pred_]\n",
    "imgs_pred_ = np.array(imgs_pred_)\n",
    "imgs_pred_= imgs_pred_.reshape((len(imgs_pred_),48,48,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad51f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(imgs_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(state_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beb9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(imgs_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0203ae77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4598990",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.argmax(axis = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2183a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats= y_dummies.columns\n",
    "cats = [x.replace(\"0_\",\"\") for x in cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eb031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d087b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07eefd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "states_model = []\n",
    "i=0\n",
    "for i in range(len(prediction)):\n",
    "    states_model.append(cats[prediction[i].argmax()])\n",
    "    print(state_pred[i],cats[prediction[i].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839cc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a8cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(state_pred,states_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33581e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e2eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(states_model, state_pred)\n",
    "conf = pd.DataFrame(conf,columns=cats, index=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9930098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "heatmap = sns.heatmap(conf, annot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec979b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5734112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
