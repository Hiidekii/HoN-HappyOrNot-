{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db5d3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import face_recognition as fr\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import cv2\n",
    "face_cascade = cv2.CascadeClassifier('gui_2/assets/haarcascade_frontalface_default.xml')\n",
    "HEIGHT, WIDTH =48,48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a9ac7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"data/all/train/\",\"data/all/test/\"] #[\"data/CK+/\",\"data/fer/train/\"]\n",
    "# data  = listdir(path)\n",
    "ignore = [\"morralla\",\".DS_Store\",\"contempt\"]\n",
    "imgs = []\n",
    "state = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "355c9697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in paths:\n",
    "#     for item in listdir(path):\n",
    "#         if item not in ignore:\n",
    "#             imgs.extend([f\"{path}{item}/{p}\" for p in listdir(f\"{path}{item}\")])\n",
    "#             state.extend([item for p in listdir(f\"{path}{item}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1a6c9fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50964, 50964)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs),len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "313463ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
    "# dif_collections= []\n",
    "# for emotion in cats:\n",
    "#     dic = {\"x\":[],\"y\":[]}\n",
    "#     for i in range(len(state)):\n",
    "#         if state[i] == emotion:\n",
    "#             dic[\"x\"].append(imgs[i])\n",
    "#             dic[\"y\"].append(emotion)\n",
    "#     dif_collections.append(dic)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "14e5cd92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5305, 5305)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dif_collections[0][\"x\"]),len(dif_collections[0][\"y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad0c14b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    for item in listdir(path):\n",
    "        if item not in ignore:\n",
    "            imgs.extend([{\"path\": f\"{path}{item}/{p}\", \"emotion\": item}for p in listdir(f\"{path}{item}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32166a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_ = []\n",
    "state = []\n",
    "for p in imgs:\n",
    "    temp = cv2.imread(p[\"path\"],0)\n",
    "    faces = face_cascade.detectMultiScale(temp, 1.1, 5)\n",
    "    if len(faces)==1:\n",
    "        for (x,y,w,h) in faces:\n",
    "            recortada = temp[y:y+h, x:x+w]\n",
    "            recortada = cv2.resize(recortada,(HEIGHT, WIDTH))\n",
    "            imgs_.append(recortada)\n",
    "            state.append(p[\"emotion\"])\n",
    "    \n",
    "        \n",
    "imgs_ = [el/255 for el in imgs_]\n",
    "imgs_array = np.array(imgs_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbcb58bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22087, 48, 48)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e892e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# errs = []\n",
    "# try:\n",
    "#     imgs_ = []\n",
    "#     for p in imgs:\n",
    "#         temp = Image.open(p)\n",
    "#         save = temp.copy()\n",
    "#         imgs_.append(save)\n",
    "#         temp.close()\n",
    "# except:\n",
    "#     errs.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a50273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_array = []\n",
    "# HEIGHT, WIDTH =48,48\n",
    "# for f in imgs_:\n",
    "#     img = f.convert(\"L\").resize((HEIGHT, WIDTH))\n",
    "#     imgs_array.append(np.array(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79f7f74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs_array = [el/255 for el in imgs_array]\n",
    "# imgs_array = np.array(imgs_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad18b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_array= imgs_array.reshape((len(imgs_array),48,48,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7808de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22087, 48, 48, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc84d548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b65e3bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(state)\n",
    "y_dummies = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "f5764951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_anger</th>\n",
       "      <th>0_disgust</th>\n",
       "      <th>0_fear</th>\n",
       "      <th>0_happiness</th>\n",
       "      <th>0_neutral</th>\n",
       "      <th>0_sadness</th>\n",
       "      <th>0_surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22147</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22148</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22149</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22150</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22151</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22152 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0_anger  0_disgust  0_fear  0_happiness  0_neutral  0_sadness  \\\n",
       "0            0          0       0            0          0          0   \n",
       "1            0          0       0            0          0          0   \n",
       "2            0          0       0            0          0          0   \n",
       "3            0          0       0            0          0          0   \n",
       "4            0          0       0            0          0          0   \n",
       "...        ...        ...     ...          ...        ...        ...   \n",
       "22147        0          0       0            0          0          1   \n",
       "22148        0          0       0            0          0          1   \n",
       "22149        0          0       0            0          0          1   \n",
       "22150        0          0       0            0          0          1   \n",
       "22151        0          0       0            0          0          1   \n",
       "\n",
       "       0_surprise  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               1  \n",
       "4               1  \n",
       "...           ...  \n",
       "22147           0  \n",
       "22148           0  \n",
       "22149           0  \n",
       "22150           0  \n",
       "22151           0  \n",
       "\n",
       "[22152 rows x 7 columns]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "335e6700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgs_array,y_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b5d459a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten,BatchNormalization,Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D, Conv2D\n",
    "from keras.layers.convolutional import MaxPooling1D,MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam, Nadam, SGD, Adamax\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9db01d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt = Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "751fe068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 10:12:55.495982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 10:12:55.500440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 10:12:55.500812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 10:12:55.501621: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-16 10:12:55.503007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 10:12:55.503351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 10:12:55.503682: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 10:12:56.543518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 10:12:56.543907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 10:12:56.544233: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-16 10:12:56.544577: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5890 MB memory:  -> device: 0, name: GeForce RTX 2070 SUPER, pci bus id: 0000:08:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# 1-conv\n",
    "model.add(Conv2D(64,(3,3),padding='same',input_shape = (48,48,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 2-conv\n",
    "model.add(Conv2D(128,(5,5),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 3-conv\n",
    "model.add(Conv2D(512,(3,3),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# 4-conv\n",
    "model.add(Conv2D(512,(3,3),padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(7,activation='softmax'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0005)\n",
    "\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ed763968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "468/468 [==============================] - 164s 349ms/step - loss: 1.3120 - accuracy: 0.5623 - val_loss: 1.7550 - val_accuracy: 0.5523\n",
      "Epoch 2/5000\n",
      "468/468 [==============================] - 7s 15ms/step - loss: 1.0201 - accuracy: 0.6705 - val_loss: 0.8999 - val_accuracy: 0.7034\n",
      "Epoch 3/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.8980 - accuracy: 0.7010 - val_loss: 0.9983 - val_accuracy: 0.6781\n",
      "Epoch 4/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.8206 - accuracy: 0.7225 - val_loss: 0.9735 - val_accuracy: 0.7136\n",
      "Epoch 5/5000\n",
      "468/468 [==============================] - 6s 13ms/step - loss: 0.7671 - accuracy: 0.7389 - val_loss: 0.8901 - val_accuracy: 0.7262\n",
      "Epoch 6/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.7236 - accuracy: 0.7483 - val_loss: 0.7286 - val_accuracy: 0.7635\n",
      "Epoch 7/5000\n",
      "468/468 [==============================] - 6s 13ms/step - loss: 0.6974 - accuracy: 0.7574 - val_loss: 0.7686 - val_accuracy: 0.7389\n",
      "Epoch 8/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.6607 - accuracy: 0.7692 - val_loss: 0.8412 - val_accuracy: 0.7076\n",
      "Epoch 9/5000\n",
      "468/468 [==============================] - 6s 13ms/step - loss: 0.6381 - accuracy: 0.7733 - val_loss: 0.7567 - val_accuracy: 0.7527\n",
      "Epoch 10/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.5963 - accuracy: 0.7902 - val_loss: 0.8282 - val_accuracy: 0.7539\n",
      "Epoch 11/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.5676 - accuracy: 0.7982 - val_loss: 0.6780 - val_accuracy: 0.7834\n",
      "Epoch 12/5000\n",
      "468/468 [==============================] - 5s 12ms/step - loss: 0.5508 - accuracy: 0.8020 - val_loss: 0.8169 - val_accuracy: 0.7677\n",
      "Epoch 13/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.5168 - accuracy: 0.8175 - val_loss: 0.8655 - val_accuracy: 0.7653\n",
      "Epoch 14/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.4784 - accuracy: 0.8256 - val_loss: 0.8127 - val_accuracy: 0.7473\n",
      "Epoch 15/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.4621 - accuracy: 0.8330 - val_loss: 0.7971 - val_accuracy: 0.7690\n",
      "Epoch 16/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.4357 - accuracy: 0.8427 - val_loss: 0.6472 - val_accuracy: 0.8002\n",
      "Epoch 17/5000\n",
      "468/468 [==============================] - 5s 12ms/step - loss: 0.4013 - accuracy: 0.8536 - val_loss: 0.7626 - val_accuracy: 0.7708\n",
      "Epoch 18/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.3863 - accuracy: 0.8589 - val_loss: 0.7677 - val_accuracy: 0.7708\n",
      "Epoch 19/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.3544 - accuracy: 0.8740 - val_loss: 1.1440 - val_accuracy: 0.7521\n",
      "Epoch 20/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.3250 - accuracy: 0.8832 - val_loss: 0.7164 - val_accuracy: 0.7864\n",
      "Epoch 21/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.3240 - accuracy: 0.8830 - val_loss: 0.9838 - val_accuracy: 0.7605\n",
      "Epoch 22/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.2828 - accuracy: 0.8981 - val_loss: 0.9166 - val_accuracy: 0.7557\n",
      "Epoch 23/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.2715 - accuracy: 0.9025 - val_loss: 0.8929 - val_accuracy: 0.7882\n",
      "Epoch 24/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.2579 - accuracy: 0.9079 - val_loss: 0.9092 - val_accuracy: 0.7401\n",
      "Epoch 25/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.2492 - accuracy: 0.9118 - val_loss: 1.2907 - val_accuracy: 0.7762\n",
      "Epoch 26/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.2323 - accuracy: 0.9160 - val_loss: 0.8270 - val_accuracy: 0.7611\n",
      "Epoch 27/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.2213 - accuracy: 0.9191 - val_loss: 0.8867 - val_accuracy: 0.7744\n",
      "Epoch 28/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.2012 - accuracy: 0.9270 - val_loss: 0.9955 - val_accuracy: 0.7503\n",
      "Epoch 29/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.2062 - accuracy: 0.9267 - val_loss: 0.8809 - val_accuracy: 0.7527\n",
      "Epoch 30/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.1845 - accuracy: 0.9367 - val_loss: 1.0327 - val_accuracy: 0.7593\n",
      "Epoch 31/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.1771 - accuracy: 0.9350 - val_loss: 1.0906 - val_accuracy: 0.7617\n",
      "Epoch 32/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1664 - accuracy: 0.9401 - val_loss: 0.9480 - val_accuracy: 0.7978\n",
      "Epoch 33/5000\n",
      "468/468 [==============================] - 6s 12ms/step - loss: 0.1634 - accuracy: 0.9422 - val_loss: 1.0691 - val_accuracy: 0.7647\n",
      "Epoch 34/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1583 - accuracy: 0.9453 - val_loss: 1.3893 - val_accuracy: 0.7647\n",
      "Epoch 35/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1505 - accuracy: 0.9468 - val_loss: 1.1880 - val_accuracy: 0.7906\n",
      "Epoch 36/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1413 - accuracy: 0.9492 - val_loss: 1.0650 - val_accuracy: 0.7876\n",
      "Epoch 37/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1363 - accuracy: 0.9489 - val_loss: 1.0134 - val_accuracy: 0.7413\n",
      "Epoch 38/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1301 - accuracy: 0.9537 - val_loss: 1.0262 - val_accuracy: 0.7756\n",
      "Epoch 39/5000\n",
      "468/468 [==============================] - 5s 12ms/step - loss: 0.1328 - accuracy: 0.9536 - val_loss: 1.2765 - val_accuracy: 0.7792\n",
      "Epoch 40/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1241 - accuracy: 0.9555 - val_loss: 1.1251 - val_accuracy: 0.7617\n",
      "Epoch 41/5000\n",
      "468/468 [==============================] - 5s 12ms/step - loss: 0.1181 - accuracy: 0.9585 - val_loss: 1.1753 - val_accuracy: 0.7587\n",
      "Epoch 42/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1152 - accuracy: 0.9592 - val_loss: 1.3238 - val_accuracy: 0.7702\n",
      "Epoch 43/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1200 - accuracy: 0.9599 - val_loss: 1.3789 - val_accuracy: 0.7744\n",
      "Epoch 44/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1073 - accuracy: 0.9621 - val_loss: 1.0442 - val_accuracy: 0.7822\n",
      "Epoch 45/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1066 - accuracy: 0.9626 - val_loss: 1.1233 - val_accuracy: 0.7720\n",
      "Epoch 46/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.1056 - accuracy: 0.9631 - val_loss: 1.0811 - val_accuracy: 0.7485\n",
      "Epoch 47/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.0959 - accuracy: 0.9644 - val_loss: 1.7126 - val_accuracy: 0.7503\n",
      "Epoch 48/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.0983 - accuracy: 0.9635 - val_loss: 1.1705 - val_accuracy: 0.7587\n",
      "Epoch 49/5000\n",
      "468/468 [==============================] - 5s 12ms/step - loss: 0.0882 - accuracy: 0.9698 - val_loss: 1.0804 - val_accuracy: 0.7762\n",
      "Epoch 50/5000\n",
      "468/468 [==============================] - 5s 11ms/step - loss: 0.0861 - accuracy: 0.9698 - val_loss: 1.4472 - val_accuracy: 0.7738\n",
      "Epoch 51/5000\n",
      "287/468 [=================>............] - ETA: 2s - loss: 0.0845 - accuracy: 0.9704"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9892/3435792529.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mjson_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodelo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    510\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model, model_from_json\n",
    "from keras import callbacks, optimizers\n",
    "import tensorflow as tf\n",
    "from datetime import date\n",
    "\n",
    "fecha=str(date.today().year)+str(date.today().month)+str(date.today().day)    \n",
    "symbol = 'alldata'\n",
    "h5 = symbol + '_2_' + fecha + '_v4.h5'\n",
    "checkpoint = callbacks.ModelCheckpoint(h5,\n",
    "                                       monitor='loss',\n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True,\n",
    "                                       #save_weights_only=True,\n",
    "                                       mode='auto',\n",
    "                                       save_freq=1)\n",
    "callback = [checkpoint]\n",
    "json = symbol + '_best_model' + fecha + '.json'\n",
    "model_json = model.to_json()\n",
    "with open(json, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "modelo = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs = 5000,callbacks = callback,validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54d00c25",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<keras.engine.sequential.Sequential object at 0x7f3d6b69cfd0>' (type <class 'keras.engine.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23374/1739304912.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mimgs_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;31m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# of spawning threads.  See joblib issue #112.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(delayed(_fit_binary)(\n\u001b[0m\u001b[1;32m    282\u001b[0m             self.estimator, X, column, classes=[\n\u001b[1;32m    283\u001b[0m                 \u001b[0;34m\"not %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[0;34m(estimator, X, y, classes)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ConstantPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/core/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                 \"scikit-learn estimator instead of a class.\")\n\u001b[1;32m     66\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 raise TypeError(\"Cannot clone object '%s' (type %s): \"\n\u001b[0m\u001b[1;32m     68\u001b[0m                                 \u001b[0;34m\"it does not seem to be a scikit-learn \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                 \u001b[0;34m\"estimator as it does not implement a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<keras.engine.sequential.Sequential object at 0x7f3d6b69cfd0>' (type <class 'keras.engine.sequential.Sequential'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "X =  imgs_array.copy()\n",
    "y = state.copy()\n",
    "clf = OneVsRestClassifier(model).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5dbceb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"data/predict/\"]\n",
    "# data_pred  = listdir(path)\n",
    "ignore = [\"morralla\",\".DS_Store\",\"contempt\"]\n",
    "imgs_pred = []\n",
    "state_pred = []\n",
    "im_pred = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b355f5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in paths:\n",
    "    for item in listdir(path):\n",
    "        if item not in ignore:\n",
    "            imgs_pred.extend([{\"path\": f\"{path}{item}/{p}\", \"emotion\": item}for p in listdir(f\"{path}{item}\")])\n",
    "            #state_pred.extend([item for p in listdir(f\"{path}{item}\")])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "31d91a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "22f61c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "777cc70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('alldata_2_20211115.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e98ad3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "22327b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_pred_ = []\n",
    "state_pred = []\n",
    "for p in imgs_pred:\n",
    "    temp = cv2.imread(p[\"path\"],0)\n",
    "    faces = face_cascade.detectMultiScale(temp, 1.1, 5)\n",
    "    if len(faces)>0:\n",
    "        for (x,y,w,h) in faces:\n",
    "            recortada = temp[y:y+h, x:x+w]\n",
    "            recortada = cv2.resize(recortada,(HEIGHT, WIDTH))\n",
    "            imgs_pred_.append(recortada)\n",
    "            state_pred.append(p[\"emotion\"])\n",
    "    \n",
    "        \n",
    "imgs_pred_ = [el/255 for el in imgs_pred_]\n",
    "imgs_pred_ = np.array(imgs_pred_)\n",
    "imgs_pred_= imgs_pred_.reshape((len(imgs_pred_),48,48,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5ad51f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(imgs_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ac6f59d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1beb9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(imgs_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0203ae77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'surprise',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'disgust',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'anger',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'neutral',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'happiness',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'fear',\n",
       " 'sadness',\n",
       " 'sadness',\n",
       " 'sadness',\n",
       " 'sadness',\n",
       " 'sadness',\n",
       " 'sadness',\n",
       " 'sadness']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e4598990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.4894957e-05, 3.7728207e-09, 6.8917363e-03, 2.5523326e-08,\n",
       "        5.1553252e-07, 1.6033631e-07, 9.9302262e-01],\n",
       "       [1.0018654e-08, 1.5192790e-12, 1.0942463e-05, 9.6037495e-01,\n",
       "        4.1263547e-07, 5.1037152e-12, 3.9613694e-02],\n",
       "       [4.5495493e-05, 5.8931235e-08, 2.1744058e-02, 4.6597645e-01,\n",
       "        1.6718764e-02, 2.0592222e-04, 4.9530926e-01],\n",
       "       [2.2900608e-08, 2.4792980e-12, 1.6108788e-07, 6.3151351e-10,\n",
       "        8.7809113e-08, 4.0679465e-12, 9.9999964e-01],\n",
       "       [1.9551955e-07, 1.1681860e-10, 8.5411684e-06, 9.5126493e-09,\n",
       "        2.8508299e-07, 1.5838350e-11, 9.9999106e-01]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ddb2d9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 3, 6, 6, 6, 6, 4, 6, 6, 0, 6, 6, 6, 6, 4, 0, 1, 3, 0, 5, 4, 1,\n",
       "       1, 1, 1, 4, 1, 4, 2, 4, 1, 1, 4, 0, 0, 0, 0, 0, 4, 0, 4, 4, 4, 4,\n",
       "       3, 4, 5, 3, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 0, 1, 0,\n",
       "       1, 0, 4, 2, 6, 6, 6, 4, 5, 5, 4, 5, 5, 4])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.argmax(axis = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2f2183a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_dummies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7009/3596858499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcats\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0my_dummies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0_\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_dummies' is not defined"
     ]
    }
   ],
   "source": [
    "cats= y_dummies.columns\n",
    "cats = [x.replace(\"0_\",\"\") for x in cats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "30eb031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "07d087b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b07eefd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surprise surprise\n",
      "surprise happiness\n",
      "surprise surprise\n",
      "surprise surprise\n",
      "surprise surprise\n",
      "surprise surprise\n",
      "surprise neutral\n",
      "surprise surprise\n",
      "surprise surprise\n",
      "surprise anger\n",
      "surprise surprise\n",
      "surprise surprise\n",
      "surprise surprise\n",
      "surprise surprise\n",
      "disgust neutral\n",
      "disgust anger\n",
      "disgust disgust\n",
      "disgust happiness\n",
      "disgust anger\n",
      "disgust sadness\n",
      "disgust neutral\n",
      "disgust disgust\n",
      "disgust disgust\n",
      "disgust disgust\n",
      "disgust disgust\n",
      "disgust neutral\n",
      "disgust disgust\n",
      "disgust neutral\n",
      "disgust fear\n",
      "disgust neutral\n",
      "disgust disgust\n",
      "disgust disgust\n",
      "disgust neutral\n",
      "anger anger\n",
      "anger anger\n",
      "anger anger\n",
      "anger anger\n",
      "anger anger\n",
      "anger neutral\n",
      "anger anger\n",
      "neutral neutral\n",
      "neutral neutral\n",
      "neutral neutral\n",
      "neutral neutral\n",
      "neutral happiness\n",
      "neutral neutral\n",
      "neutral sadness\n",
      "neutral happiness\n",
      "neutral neutral\n",
      "neutral neutral\n",
      "neutral neutral\n",
      "happiness happiness\n",
      "happiness happiness\n",
      "happiness happiness\n",
      "happiness happiness\n",
      "happiness happiness\n",
      "happiness happiness\n",
      "happiness happiness\n",
      "happiness happiness\n",
      "happiness happiness\n",
      "happiness happiness\n",
      "happiness happiness\n",
      "fear surprise\n",
      "fear anger\n",
      "fear disgust\n",
      "fear anger\n",
      "fear disgust\n",
      "fear anger\n",
      "fear neutral\n",
      "fear fear\n",
      "fear surprise\n",
      "fear surprise\n",
      "fear surprise\n",
      "sadness neutral\n",
      "sadness sadness\n",
      "sadness sadness\n",
      "sadness neutral\n",
      "sadness sadness\n",
      "sadness sadness\n",
      "sadness neutral\n"
     ]
    }
   ],
   "source": [
    "states_model = []\n",
    "i=0\n",
    "for i in range(len(prediction)):\n",
    "    states_model.append(cats[prediction[i].argmax()])\n",
    "    print(state_pred[i],cats[prediction[i].argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "839cc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f8a8cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7625"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(state_pred,states_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33581e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "35ed4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(states_model, state_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c1b0722e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "heatmap = sns.heatmap(conf, annot=True)\n",
    "\n",
    "['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34457c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d721b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
